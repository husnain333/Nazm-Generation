{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10761458,"sourceType":"datasetVersion","datasetId":6675282}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport pickle\nfrom collections import Counter\nimport time\nfrom tqdm import tqdm\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:05.450661Z","iopub.execute_input":"2025-03-05T17:17:05.450926Z","iopub.status.idle":"2025-03-05T17:17:11.325490Z","shell.execute_reply.started":"2025-03-05T17:17:05.450900Z","shell.execute_reply":"2025-03-05T17:17:11.324323Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/romanmy/Roman-Urdu-Poetry.csv')\ntexts = df['Poetry'].dropna().tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:16.494478Z","iopub.execute_input":"2025-03-05T17:17:16.494843Z","iopub.status.idle":"2025-03-05T17:17:16.571008Z","shell.execute_reply.started":"2025-03-05T17:17:16.494817Z","shell.execute_reply":"2025-03-05T17:17:16.569827Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"avg_word_length = sum(len(sentence.split()) for sentence in texts) / len(texts)\nprint(f\"Average length (in words): {avg_word_length:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:21.299447Z","iopub.execute_input":"2025-03-05T17:17:21.299870Z","iopub.status.idle":"2025-03-05T17:17:21.323740Z","shell.execute_reply.started":"2025-03-05T17:17:21.299840Z","shell.execute_reply":"2025-03-05T17:17:21.322483Z"}},"outputs":[{"name":"stdout","text":"Average length (in words): 119.43\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"class PoetryDataset(Dataset):\n    def __init__(self, sequences, targets):\n        self.sequences = sequences\n        self.targets = targets\n        \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        return self.sequences[idx], self.targets[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:21.888764Z","iopub.execute_input":"2025-03-05T17:17:21.889154Z","iopub.status.idle":"2025-03-05T17:17:21.895168Z","shell.execute_reply.started":"2025-03-05T17:17:21.889124Z","shell.execute_reply":"2025-03-05T17:17:21.893538Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Preprocessing with improved cleaning\ndef preprocess_text(text):\n    text = text.lower()\n    text = text.replace('\\n', ' [NEWLINE] ')  # Preserve line breaks\n    text = ''.join([c for c in text if c.isalpha() or c in [\" \", \"'\", \".\", \"[\", \"]\"]])\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:24.288776Z","iopub.execute_input":"2025-03-05T17:17:24.289250Z","iopub.status.idle":"2025-03-05T17:17:24.294934Z","shell.execute_reply.started":"2025-03-05T17:17:24.289215Z","shell.execute_reply":"2025-03-05T17:17:24.293598Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def preprocess_text(text):\n    text = text.lower()\n    text = text.replace('\\n', ' [NEWLINE] ')\n    text = ''.join([c for c in text if c.isalpha() or c in [\" \", \"'\", \".\", \"[\", \"]\"]])\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:26.274612Z","iopub.execute_input":"2025-03-05T17:17:26.274978Z","iopub.status.idle":"2025-03-05T17:17:26.280301Z","shell.execute_reply.started":"2025-03-05T17:17:26.274949Z","shell.execute_reply":"2025-03-05T17:17:26.279123Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"processed_texts = [preprocess_text(t) for t in texts]\ncorpus = \" \".join(processed_texts).split()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:27.920911Z","iopub.execute_input":"2025-03-05T17:17:27.921270Z","iopub.status.idle":"2025-03-05T17:17:28.092012Z","shell.execute_reply.started":"2025-03-05T17:17:27.921244Z","shell.execute_reply":"2025-03-05T17:17:28.090881Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Build vocabulary\nword_counts = Counter(corpus)\nvocab = sorted(word_counts, key=word_counts.get, reverse=True)  # Keep all words\nvocab = ['<PAD>', '<UNK>'] + vocab  # Add special tokens\nword2idx = {word: idx for idx, word in enumerate(vocab)}\n\nprint(f\"Vocabulary size: {len(vocab)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:29.542731Z","iopub.execute_input":"2025-03-05T17:17:29.543130Z","iopub.status.idle":"2025-03-05T17:17:29.582294Z","shell.execute_reply.started":"2025-03-05T17:17:29.543086Z","shell.execute_reply":"2025-03-05T17:17:29.581247Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 17242\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Save vocabulary\nwith open('word2idx.pkl', 'wb') as f:\n    pickle.dump(word2idx, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:19:08.682632Z","iopub.execute_input":"2025-03-05T17:19:08.683008Z","iopub.status.idle":"2025-03-05T17:19:08.693464Z","shell.execute_reply.started":"2025-03-05T17:19:08.682979Z","shell.execute_reply":"2025-03-05T17:19:08.692411Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Create sequences\nsequence_length = 150\nsequences = []\ntargets = []\nfor i in range(sequence_length, len(corpus)):\n    seq = corpus[i-sequence_length:i]\n    target = corpus[i]\n    sequences.append([word2idx.get(word, word2idx['<UNK>']) for word in seq])\n    targets.append(word2idx.get(target, word2idx['<UNK>']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:34.154673Z","iopub.execute_input":"2025-03-05T17:17:34.155115Z","iopub.status.idle":"2025-03-05T17:17:38.878786Z","shell.execute_reply.started":"2025-03-05T17:17:34.155048Z","shell.execute_reply":"2025-03-05T17:17:38.877486Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Split data\nX_train, X_val, y_train, y_val = train_test_split(sequences, targets, test_size=0.05, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:38.880421Z","iopub.execute_input":"2025-03-05T17:17:38.880831Z","iopub.status.idle":"2025-03-05T17:17:39.012409Z","shell.execute_reply.started":"2025-03-05T17:17:38.880791Z","shell.execute_reply":"2025-03-05T17:17:39.011265Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Create DataLoaders\ndef collate_fn(batch):\n    sequences, targets = zip(*batch)\n    sequences = pad_sequence([torch.LongTensor(seq) for seq in sequences], \n                           batch_first=True, padding_value=word2idx['<PAD>'])\n    targets = torch.LongTensor(targets)\n    return sequences, targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:39.014462Z","iopub.execute_input":"2025-03-05T17:17:39.014872Z","iopub.status.idle":"2025-03-05T17:17:39.021462Z","shell.execute_reply.started":"2025-03-05T17:17:39.014841Z","shell.execute_reply":"2025-03-05T17:17:39.019502Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_dataset = PoetryDataset(X_train, y_train)\nval_dataset = PoetryDataset(X_val, y_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:39.022732Z","iopub.execute_input":"2025-03-05T17:17:39.023146Z","iopub.status.idle":"2025-03-05T17:17:39.040523Z","shell.execute_reply.started":"2025-03-05T17:17:39.023111Z","shell.execute_reply":"2025-03-05T17:17:39.039168Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"batch_size = 128\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:42.087387Z","iopub.execute_input":"2025-03-05T17:17:42.087758Z","iopub.status.idle":"2025-03-05T17:17:42.093596Z","shell.execute_reply.started":"2025-03-05T17:17:42.087730Z","shell.execute_reply":"2025-03-05T17:17:42.092237Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class RMSNorm(nn.Module):\n    def __init__(self, dim, eps=1e-8):\n        \"\"\"\n        RMSNorm normalizes inputs by their root-mean-square.\n        \"\"\"\n        super(RMSNorm, self).__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(dim))\n    \n    def forward(self, x):\n        # x shape: (..., dim)\n        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n        return self.scale * (x / rms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:44.151345Z","iopub.execute_input":"2025-03-05T17:17:44.151666Z","iopub.status.idle":"2025-03-05T17:17:44.158077Z","shell.execute_reply.started":"2025-03-05T17:17:44.151642Z","shell.execute_reply":"2025-03-05T17:17:44.156657Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ROPE (Rotary Positional Embedding) Functions\n########################################\ndef get_rope_embeddings(seq_len, dim, device):\n    \"\"\"\n    Compute sin and cos embeddings for ROPE.\n    Assumes dim is even.\n    \"\"\"\n    # Compute inverse frequency for each even dimension\n    inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=device).float() / dim))\n    positions = torch.arange(seq_len, device=device).float()  \n    sinusoid_inp = torch.outer(positions, inv_freq)           \n    sin = torch.sin(sinusoid_inp)  \n    cos = torch.cos(sinusoid_inp)  \n    # Expand to match original dim by interleaving sin and cos along last dimension.\n    # One simple approach is to repeat each column twice.\n    sin = torch.stack([sin, sin], dim=-1).reshape(seq_len, dim)\n    cos = torch.stack([cos, cos], dim=-1).reshape(seq_len, dim)\n    return sin, cos","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:46.120309Z","iopub.execute_input":"2025-03-05T17:17:46.120696Z","iopub.status.idle":"2025-03-05T17:17:46.127002Z","shell.execute_reply.started":"2025-03-05T17:17:46.120668Z","shell.execute_reply":"2025-03-05T17:17:46.125725Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def apply_rope(x):\n    \"\"\"\n    Applies Rotary Positional Embedding (ROPE) to input x.\n    x: Tensor of shape (batch, seq_len, dim) where dim is even.\n    \"\"\"\n    batch, seq_len, dim = x.shape\n    # Get sin and cos for the current sequence length\n    sin, cos = get_rope_embeddings(seq_len, dim, x.device)  # each: (seq_len, dim)\n    # Expand to match batch size\n    sin = sin.unsqueeze(0)  # (1, seq_len, dim)\n    cos = cos.unsqueeze(0)  # (1, seq_len, dim)\n    \n    # ROPE is typically applied per pair of dimensions.\n    # Here we split the last dim into even and odd indices:\n    x1 = x[..., ::2]  # (batch, seq_len, dim/2)\n    x2 = x[..., 1::2] # (batch, seq_len, dim/2)\n    \n    # Similarly for sin and cos (also split in half)\n    sin_half = sin[..., ::2]\n    cos_half = cos[..., ::2]\n    \n    # Apply the rotation\n    x1_rot = x1 * cos_half - x2 * sin_half\n    x2_rot = x1 * sin_half + x2 * cos_half\n    # Reconstruct interleaved tensor\n    x_rot = torch.stack((x1_rot, x2_rot), dim=-1).reshape(batch, seq_len, dim)\n    return x_rot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:17:48.431738Z","iopub.execute_input":"2025-03-05T17:17:48.432175Z","iopub.status.idle":"2025-03-05T17:17:48.439493Z","shell.execute_reply.started":"2025-03-05T17:17:48.432145Z","shell.execute_reply":"2025-03-05T17:17:48.438172Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Pretrained LSTM Poetry Model with ROPE and RMSNorm\n########################################\nclass PretrainedPoetryLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512, num_layers=2):\n        \"\"\"\n        A poetry language model using a pretrained (unsupervised) LSTM,\n        enhanced with ROPE and RMSNorm.\n        \"\"\"\n        super(PretrainedPoetryLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=word2idx['<PAD>'])\n        # LSTM is bidirectional. (You can change to uni-directional if desired.)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n                            batch_first=True, dropout=0.1, bidirectional=True)\n        # RMSNorm applied on the output (hidden dim doubled due to bidirectionality)\n        self.lms_norm = RMSNorm(hidden_dim * 2)\n        # Fully connected layers for prediction\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_dim * 2, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            RMSNorm(512),\n            nn.Linear(512, vocab_size)\n        )\n    def forward(self, x):\n        # x: (batch, seq_len) integer token indices\n        emb = self.embedding(x)  # (batch, seq_len, embedding_dim)\n        # Apply ROPE to the embeddings\n        emb = apply_rope(emb)\n        # Pass embeddings through LSTM\n        lstm_out, _ = self.lstm(emb)  # (batch, seq_len, hidden_dim*2)\n        # Here, we take the last time-step’s output.\n        last_out = lstm_out[:, -1, :]  # (batch, hidden_dim*2)\n        # Normalize using RMSNorm\n        normed_out = self.lms_norm(last_out)\n        # Fully connected layers produce logits for next-token prediction.\n        logits = self.fc(normed_out)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:18:02.107755Z","iopub.execute_input":"2025-03-05T17:18:02.108219Z","iopub.status.idle":"2025-03-05T17:18:02.131163Z","shell.execute_reply.started":"2025-03-05T17:18:02.108185Z","shell.execute_reply":"2025-03-05T17:18:02.129958Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# 9. Training Loop & Model Saving\n########################################\nvocab_size = len(vocab)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = PretrainedPoetryLSTM(vocab_size, embedding_dim=256, hidden_dim=512, num_layers=2).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:18:03.885742Z","iopub.execute_input":"2025-03-05T17:18:03.886176Z","iopub.status.idle":"2025-03-05T17:18:04.223307Z","shell.execute_reply.started":"2025-03-05T17:18:03.886146Z","shell.execute_reply":"2025-03-05T17:18:04.222182Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.RMSprop(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:18:05.646275Z","iopub.execute_input":"2025-03-05T17:18:05.646667Z","iopub.status.idle":"2025-03-05T17:18:08.401053Z","shell.execute_reply.started":"2025-03-05T17:18:05.646640Z","shell.execute_reply":"2025-03-05T17:18:08.399927Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"best_val_loss = float('inf')\npatience = 25\ncounter = 0\nnum_epochs = 25 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:18:08.402471Z","iopub.execute_input":"2025-03-05T17:18:08.402960Z","iopub.status.idle":"2025-03-05T17:18:08.407973Z","shell.execute_reply.started":"2025-03-05T17:18:08.402935Z","shell.execute_reply":"2025-03-05T17:18:08.406474Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    start_time = time.time()\n    model.train()\n    train_loss = 0\n    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n    for inputs, targets in train_bar:\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n        optimizer.step()\n        train_loss += loss.item()\n        train_bar.set_postfix(loss=loss.item())\n    train_loss /= len(train_loader)\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == targets).sum().item()\n            total += targets.size(0)\n    val_loss /= len(val_loader)\n    val_acc = correct / total\n    epoch_time = time.time() - start_time\n    scheduler.step(val_loss)\n    \n    print(f'Epoch {epoch+1}/{num_epochs} | Time: {epoch_time:.2f}s | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n    \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'best_model.pth')\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping!\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:07:15.450714Z","iopub.execute_input":"2025-03-05T14:07:15.451015Z","iopub.status.idle":"2025-03-05T16:08:12.351881Z","shell.execute_reply.started":"2025-03-05T14:07:15.450988Z","shell.execute_reply":"2025-03-05T16:08:12.351088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the model's state dictionary\ntorch.save(model.state_dict(), 'urdu_poetry_gru.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:08:44.979953Z","iopub.execute_input":"2025-03-05T16:08:44.980268Z","iopub.status.idle":"2025-03-05T16:08:45.258620Z","shell.execute_reply.started":"2025-03-05T16:08:44.980219Z","shell.execute_reply":"2025-03-05T16:08:45.257786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx2word = {idx: word for word, idx in word2idx.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:18:31.784220Z","iopub.execute_input":"2025-03-05T17:18:31.784610Z","iopub.status.idle":"2025-03-05T17:18:31.793635Z","shell.execute_reply.started":"2025-03-05T17:18:31.784583Z","shell.execute_reply":"2025-03-05T17:18:31.792419Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Save vocabulary\nwith open('idx2word.pkl', 'wb') as f:\n    pickle.dump(idx2word, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:21:27.084779Z","iopub.execute_input":"2025-03-05T17:21:27.085215Z","iopub.status.idle":"2025-03-05T17:21:27.096904Z","shell.execute_reply.started":"2025-03-05T17:21:27.085185Z","shell.execute_reply":"2025-03-05T17:21:27.095722Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def preprocess_prompt(prompt):\n    prompt = prompt.lower()\n    prompt = prompt.replace('\\n', ' [NEWLINE] ')\n    prompt = ''.join([c for c in prompt if c.isalpha() or c in [\" \", \"'\", \".\", \"[\", \"]\"]])\n    return prompt.split()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:12:47.509628Z","iopub.execute_input":"2025-03-05T16:12:47.509947Z","iopub.status.idle":"2025-03-05T16:12:47.517667Z","shell.execute_reply.started":"2025-03-05T16:12:47.509919Z","shell.execute_reply":"2025-03-05T16:12:47.516753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_next_word(model, prompt, sequence_length=20, device='cpu'):\n    tokens = preprocess_prompt(prompt)\n    # Pad or trim the prompt to the required sequence length.\n    if len(tokens) < sequence_length:\n        tokens = ['<PAD>'] * (sequence_length - len(tokens)) + tokens\n    else:\n        tokens = tokens[-sequence_length:]\n    seq_indices = [word2idx.get(token, word2idx['<UNK>']) for token in tokens]\n    input_tensor = torch.LongTensor(seq_indices).unsqueeze(0).to(device)  # (1, sequence_length)\n    model.eval()\n    with torch.no_grad():\n        logits = model(input_tensor)\n        predicted_idx = torch.argmax(logits, dim=1).item()\n    predicted_word = idx2word.get(predicted_idx, '<UNK>')\n    return predicted_word\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T17:45:00.601700Z","iopub.execute_input":"2025-02-17T17:45:00.601988Z","iopub.status.idle":"2025-02-17T17:45:00.607512Z","shell.execute_reply.started":"2025-02-17T17:45:00.601967Z","shell.execute_reply":"2025-02-17T17:45:00.606458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_text(model, prompt, gen_length=20, sequence_length=20, device='cpu'):\n    \"\"\"\n    Generate text of length `gen_length` starting from the initial prompt.\n    Args:\n        model: The trained language model.\n        prompt: The starting text prompt (string).\n        gen_length: The number of words to generate.\n        sequence_length: The fixed input sequence length required by the model.\n        device: The device (CPU/GPU) to run the inference.\n    Returns:\n        A generated text string combining the prompt and the generated words.\n    \"\"\"\n    # Preprocess the prompt into tokens.\n    tokens = preprocess_prompt(prompt)\n    generated = tokens.copy()  # Start with the initial tokens.\n    \n    model.eval()\n    with torch.no_grad():\n        for _ in range(gen_length):\n            # Ensure the current sequence has the required length.\n            if len(generated) < sequence_length:\n                current_tokens = ['<PAD>'] * (sequence_length - len(generated)) + generated\n            else:\n                current_tokens = generated[-sequence_length:]\n            \n            # Convert tokens to indices.\n            seq_indices = [word2idx.get(token, word2idx['<UNK>']) for token in current_tokens]\n            input_tensor = torch.LongTensor(seq_indices).unsqueeze(0).to(device)  # Shape: (1, sequence_length)\n            \n            # Get prediction from the model.\n            logits = model(input_tensor)\n            predicted_idx = torch.argmax(logits, dim=1).item()\n            predicted_word = idx2word.get(predicted_idx, '<UNK>')\n            \n            # Append predicted word to the generated sequence.\n            generated.append(predicted_word)\n    \n    # Combine the tokens into a string.\n    return \" \".join(generated)\n\n# Example usage:\nexample_prompt = \"wo batain ab kahan\"\ngenerated_text = generate_text(model, example_prompt, gen_length=50, sequence_length=50, device=device)\nprint(\"Generated Text:\\n\", generated_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:13:20.413835Z","iopub.execute_input":"2025-03-05T16:13:20.414117Z","iopub.status.idle":"2025-03-05T16:13:20.678598Z","shell.execute_reply.started":"2025-03-05T16:13:20.414096Z","shell.execute_reply":"2025-03-05T16:13:20.677853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sudo apt update","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:24:29.122277Z","iopub.execute_input":"2025-02-17T18:24:29.122626Z","iopub.status.idle":"2025-02-17T18:24:33.977884Z","shell.execute_reply.started":"2025-02-17T18:24:29.122599Z","shell.execute_reply":"2025-02-17T18:24:33.977064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sudo apt install gh -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:30:00.667186Z","iopub.execute_input":"2025-02-17T18:30:00.667543Z","iopub.status.idle":"2025-02-17T18:30:02.766919Z","shell.execute_reply.started":"2025-02-17T18:30:00.667502Z","shell.execute_reply":"2025-02-17T18:30:02.765829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!gh auth login --with-token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:30:11.572970Z","iopub.execute_input":"2025-02-17T18:30:11.573363Z","iopub.status.idle":"2025-02-17T18:30:50.476294Z","shell.execute_reply.started":"2025-02-17T18:30:11.573326Z","shell.execute_reply":"2025-02-17T18:30:50.475186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir my_model\n!cp urdu_poetry_gru.pth my_model/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:34:34.924025Z","iopub.execute_input":"2025-02-17T18:34:34.924383Z","iopub.status.idle":"2025-02-17T18:34:35.390534Z","shell.execute_reply.started":"2025-02-17T18:34:34.924355Z","shell.execute_reply":"2025-02-17T18:34:35.389539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip urdu_poetry.zip urdu_poetry_gru.pth\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:35:55.621394Z","iopub.execute_input":"2025-02-17T18:35:55.621726Z","iopub.status.idle":"2025-02-17T18:36:00.354124Z","shell.execute_reply.started":"2025-02-17T18:35:55.621698Z","shell.execute_reply":"2025-02-17T18:36:00.353201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip urdu_poetry.zip urdu_poetry_gru.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:37:48.630794Z","iopub.execute_input":"2025-02-17T18:37:48.631121Z","iopub.status.idle":"2025-02-17T18:37:48.783268Z","shell.execute_reply.started":"2025-02-17T18:37:48.631077Z","shell.execute_reply":"2025-02-17T18:37:48.782152Z"}},"outputs":[],"execution_count":null}]}